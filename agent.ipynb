{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "# 导入基础库\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "# 导入pytorch库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# 导入图像处理库\n",
    "import timm\n",
    "# 导入绘图库\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义网络常数\n",
    "LOG_STD_MAX = 2     # 最大标准差\n",
    "LOG_STD_MIN = -20   # 最小标准差\n",
    "\n",
    "## 自定义特征提取器\n",
    "class StateFeatureExtractor(nn.Module):\n",
    "    '''\n",
    "    特征提取器, 从state中提取特征向量, 并作为动作价值函数以及策略函数的输入\n",
    "    out_channels: 输出通道数\n",
    "    gm_size: global_map的形状。gm_size = [通道数，维度，维度，维度]\n",
    "    lm_size: local_map的形状。lm_size = [通道数，维度，维度]\n",
    "    cs_size: cs的形状(cs是一个向量)。cs_size = 维度\n",
    "    gm_output_dim: self.gm_extractor的输出维度\n",
    "    '''\n",
    "    def __init__(self, gm_out_channels, lm_out_channels, gm_size, lm_size, cs_size, gm_output_dim = 128, lm_output_dim = 512, cs_output_dim = 16, output_dim = 512):\n",
    "        super(StateFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.gm_in_channels = gm_size[0]    # global_map通道数\n",
    "        self.lm_in_channels = lm_size[0]    # local_map通道数\n",
    "        self.gm_volume = gm_size[1] * gm_size[2] * gm_size[3]   # global_map通道维度之外的三个维度的形状\n",
    "        self.lm_area = lm_size[1] * lm_size[2]  # local_map通道维度之外的两个维度的形状\n",
    "        self.gm_out_channels = gm_out_channels    # global_map输出通道数\n",
    "        self.lm_out_channels = lm_out_channels  # local_map输出通道数\n",
    "        self.cs_dim = cs_size       # cs(刀具位置和姿态)的维度\n",
    "        self.gm_output_dim = gm_output_dim  # self.gm_extractor的输出维度\n",
    "        self.lm_output_dim = lm_output_dim  # self.lm_extractor的输出维度\n",
    "        self.cs_output_dim = cs_output_dim  # self.cs_extractor的输出维度\n",
    "        self.output_dim = output_dim        # self.collective_extractor的输出维度\n",
    "\n",
    "        #! 定义global_map特征提取卷积层\n",
    "        self.gm_extractor = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=self.gm_in_channels, out_channels=self.gm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=self.gm_out_channels, out_channels=self.gm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.gm_out_channels * self.gm_volume, out_features=self.gm_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 定义local_map特征提取卷积层\n",
    "        self.lm_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.lm_in_channels, out_channels=self.lm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.lm_out_channels, out_channels=self.lm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.lm_out_channels * self.lm_area, self.lm_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 定义cs特征提取层\n",
    "        self.cs_extractor = nn.Sequential(\n",
    "            nn.Linear(self.cs_dim, self.cs_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 整合三个特征提取层\n",
    "        self.collective_extractor = nn.Sequential(\n",
    "            nn.Linear(self.gm_output_dim + self.lm_output_dim + self.cs_output_dim, self.output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        gm = x[0]\n",
    "        lm = x[1]\n",
    "        cs = x[2]\n",
    "        gm = self.gm_extractor(gm)\n",
    "        lm = self.lm_extractor(lm)\n",
    "        cs = self.cs_extractor(cs)\n",
    "        x = torch.cat((gm, lm, cs), dim=1)\n",
    "        x = self.collective_extractor(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "## 自定义动作价值函数\n",
    "class QNetWork(nn.Module):\n",
    "    '''\n",
    "    动作价值函数, 根据状态(state)输入和动作(action)输入, 输出动作价值\n",
    "    if_feature_extractor: 是否使用特征提取器。True: 使用特征提取器, False: 不使用特征提取器。默认不使用特征提取器\n",
    "    state_dim: 状态维度。如果采用特征提取器, 则特征提取器的输出维度output_dim=state_dim\n",
    "    action_dim: 动作维度。action_dim = [x位置变化, y位置变化, z位置变化, theta角度变化, phi角度变化, 主轴转速的变化, 进给速度的变化]\n",
    "    output_dim: 输出维度, 固定为1, 输出奖励值\n",
    "    feature_dim1: 第一个隐藏层维度\n",
    "    feature_dim2: 第二个隐藏层维度\n",
    "    '''\n",
    "    def __init__(self, if_feature_extractor = True, features_extractor: StateFeatureExtractor = None, state_dim = 512, action_dim = 7, output_dim = 1, hidden_dim1 = 256, hidden_dim2 = 128):\n",
    "        super(QNetWork, self).__init__()\n",
    "\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_extractor = features_extractor\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "\n",
    "        #! Q值网络\n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.action_dim, self.hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim1, self.hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim2, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        if self.if_feature_extractor:   # 采用特征提取器, 将state转换成特征向量state\n",
    "            #* 动作价值函数与策略函数共用一个特征提取器\n",
    "            with torch.no_grad():\n",
    "                state = self.features_extractor(state)\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = self.q_network(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "## 自定义策略函数\n",
    "class PolicyNetWork(nn.Module):\n",
    "    '''\n",
    "    策略函数, 根据状态(state)输入, 输出动作(action)概率\n",
    "    if_feature_extractor: 是否使用特征提取器。True: 使用特征提取器, False: 不使用特征提取器。默认不使用特征提取器\n",
    "    state_dim: 状态维度\n",
    "    action_dim: 动作维度\n",
    "    feature_dim1: 第一个隐藏层维度\n",
    "    feature_dim2: 第二个隐藏层维度\n",
    "    '''\n",
    "    def __init__(self, if_feature_extractor = True, features_extractor: StateFeatureExtractor = None, state_dim = 512, action_dim = 7, hidden_dim = 256, output_dim = 128):\n",
    "        super(PolicyNetWork, self).__init__()\n",
    "\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_extractor = features_extractor\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        #! 策略网络\n",
    "        self.policy_dist = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 输出均值和标准差\n",
    "        # TODO：https://arxiv.org/abs/2005.05719 提出一种根据状态的采样策略，可以尝试使用\n",
    "        self.log_std = nn.Linear(self.output_dim, self.action_dim)\n",
    "        self.mu = nn.Linear(self.output_dim, self.action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        前向传播\n",
    "        '''\n",
    "        # 0. 特征提取器提取特征\n",
    "        if self.if_feature_extractor:\n",
    "            state = self.features_extractor(state)\n",
    "        # 1. 通过policy网络得到log_std和mu\n",
    "        x = self.policy_dist(state)\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)    # 限制log_std的范围\n",
    "        std = torch.exp(log_std)    # 得到真正的标准差\n",
    "        # 2. 通过std和mu创建高斯分布\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        # 3. 从高斯分布中取样得到动作\n",
    "        u = dist.rsample()\n",
    "        action = torch.tanh(u)  # 将动作限制在[-1， 1]之间\n",
    "        # 4. 计算动作分布的标准差并返回对数标准差的和\n",
    "        log_prob = dist.log_prob(u)\n",
    "        log_prob -= torch.log(1 - action ** 2 + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        return action, log_prob\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        采样动作\n",
    "        '''\n",
    "        # 0. 特征提取器提取特征\n",
    "        if self.if_feature_extractor:\n",
    "            state = self.features_extractor(state)\n",
    "        # 1. 通过policy网络得到log_std和mu\n",
    "        x = self.policy_dist(state)\n",
    "        mu = self.mu(x)\n",
    "        return torch.tanh(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 经验回放缓冲区\n",
    "# TODO: 优化经验回放缓冲区\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state','reward', 'done', 'action', 'next_state'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    '''\n",
    "    经验回放缓冲区\n",
    "    memory_size: 缓冲区大小\n",
    "    batch_size: 批次大小\n",
    "    '''\n",
    "    def __init__(self, memory_size = 1000000, batch_size = 256):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "\n",
    "    def add(self, state, reward, done, action, next_state):\n",
    "        self.memory.append(Transition(state, reward, done, action, next_state))\n",
    "        if len(self.memory) > self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "        assert len(self.memory) <= self.memory_size\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAC算法\n",
    "class SAC:\n",
    "    '''\n",
    "    learning_rate: 学习率\n",
    "    tau: 软更新参数\n",
    "    gamma: 折扣因子\n",
    "    buffer_size: 经验回放缓冲区大小\n",
    "    batch_size: 训练batch大小\n",
    "    if_feature_extractor: 是否使用特征提取器\n",
    "    gm_out_channels: global_map输出通道数\n",
    "    lm_out_channels: local_map输出通道数\n",
    "    gm_size: global_map的形状。gm_size = [通道数，维度，维度，维度]\n",
    "    lm_size: local_map的形状。lm_size = [通道数，维度，维度]\n",
    "    cs_size: cs的形状(cs是一个向量)。cs_size = 维度\n",
    "    gm_output_dim: self.gm_extractor的输出维度\n",
    "    lm_output_dim: self.lm_extractor的输出维度\n",
    "    cs_output_dim: self.cs_extractor的输出维度\n",
    "    features_dim: 特征提取器的输出维度\n",
    "    action_dim: 动作维度\n",
    "    q_hidden_dim1: Q网络第一个隐藏层维度\n",
    "    q_hidden_dim2: Q网络第二个隐藏层维度\n",
    "    p_hidden_dim1: 策略网络第一个隐藏层维度\n",
    "    p_hidden_dim2: 策略网络第二个隐藏层维度\n",
    "    ent_coef: 熵系数\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            learning_rate = 0.003,\n",
    "            tau = 0.005,\n",
    "            gamma = 0.99,\n",
    "            buffer_size = 1000000,\n",
    "            batch_size = 256,\n",
    "            if_feature_extractor = False,\n",
    "            gm_out_channels = 2,\n",
    "            lm_out_channels = 18,\n",
    "            gm_size = [2, 10, 10, 10],\n",
    "            lm_size = [18, 75, 75],\n",
    "            cs_size = 9,\n",
    "            gm_output_dim = 128,\n",
    "            lm_output_dim = 512,\n",
    "            cs_output_dim = 16,\n",
    "            features_dim = 512,\n",
    "            action_dim = 7,\n",
    "            q_hidden_dim1 = 256,\n",
    "            q_hidden_dim2 = 128,\n",
    "            p_hidden_dim1 = 256,\n",
    "            p_hidden_dim2 = 128,\n",
    "            ent_coef = 0.1\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_dim = features_dim    # 特征提取器的输出维度，同时也是Q网络和策略网络的状态输入维度\n",
    "        self.action_dim = action_dim        # 动作维度\n",
    "        self.q_hidden_dim1 = q_hidden_dim1  # Q网络第一个隐藏层维度\n",
    "        self.q_hidden_dim2 = q_hidden_dim2\n",
    "        self.p_hidden_dim1 = p_hidden_dim1\n",
    "        self.p_hidden_dim2 = p_hidden_dim2\n",
    "        self.target_entropy = -self.action_dim  # 目标熵\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # 使用特征提取器\n",
    "        if if_feature_extractor:\n",
    "            self.gm_out_channels = gm_out_channels\n",
    "            self.lm_out_channels = lm_out_channels\n",
    "            self.gm_size = gm_size\n",
    "            self.lm_size = lm_size\n",
    "            self.cs_size = cs_size\n",
    "            self.gm_output_dim = gm_output_dim\n",
    "            self.lm_output_dim = lm_output_dim\n",
    "            self.cs_output_dim = cs_output_dim\n",
    "            self.features_extractor = StateFeatureExtractor(gm_output_channels=self.gm_out_channels, lm_out_channels=self.lm_out_channels, \n",
    "                                                            gm_size=self.gm_size, lm_size=self.lm_size, cs_size=self.cs_size, \n",
    "                                                            gm_output_dim=self.gm_output_dim, lm_output_dim=self.lm_output_dim, cs_output_dim=self.cs_output_dim, output_dim=self.features_dim).to(self.device)\n",
    "        else:\n",
    "            self.features_extractor = None\n",
    "\n",
    "        # 定义Q网络\n",
    "        self.q1 = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor, \n",
    "                      state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q2 = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                      state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q1_target = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                             state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q2_target = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                             state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q1_opt = optim.Adam(self.q1.parameters(), lr=self.learning_rate)   # 定义Q网络优化器\n",
    "        self.q2_opt = optim.Adam(self.q2.parameters(), lr=self.learning_rate)   # 定义Q网络优化器\n",
    "\n",
    "        # 定义策略网络\n",
    "        self.policy_net = PolicyNetWork(state_dim=self.features_dim, action_dim=self.action_dim, \n",
    "                                        if_feature_extractor=self.if_feature_extractor, \n",
    "                                        hidden_dim=self.p_hidden_dim1, output_dim=self.p_hidden_dim2).to(self.device)\n",
    "        if self.if_feature_extractor:   # 采用特征提取器, 则策略网络和特征提取器共同训练\n",
    "            self.policy_opt = optim.Adam(list(self.features_extractor.parameters()) + list(self.policy_net.parameters()), lr=self.learning_rate)\n",
    "        else:\n",
    "            self.policy_opt = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)   # 定义策略网络优化器\n",
    "        \n",
    "        # 定义经验回放缓冲区\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        # 定义温度\n",
    "        self.log_ent_coef = torch.tensor([0.0]).to(self.device).requires_grad_()\n",
    "        self.ent_coef_opt = optim.Adam([self.log_ent_coef], lr=self.learning_rate)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state = torch.FloatTensor([state]).to(self.device)\n",
    "        action, _ = self.policy_net(state)\n",
    "        return action.cpu().detach().numpy()\n",
    "    \n",
    "    def eval_action(self, state):\n",
    "        state = torch.FloatTensor([state]).to(self.device)\n",
    "        action = self.policy_net.get_action(state)\n",
    "        return action.cpu().detach().numpy()\n",
    "\n",
    "    def store(self, state, reward, done, action, next_state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        reward = torch.Tensor([reward])\n",
    "        done = torch.Tensor([done])\n",
    "        action = np.array([action])\n",
    "        action = torch.Tensor(action)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        self.replay_buffer.add(state, reward, done, action, next_state)\n",
    "\n",
    "    def unpack(self, batch):\n",
    "        batch = Transition(*zip(*batch))\n",
    "        states = torch.cat(batch.state).view(self.batch_size, self.features_dim).to(self.device)\n",
    "        rewards = torch.cat(batch.reward).view(self.batch_size, 1).to(self.device)\n",
    "        dones = torch.cat(batch.done).view(self.batch_size, 1).to(self.device)\n",
    "        actions = torch.cat(batch.action).view(-1, self.action_dim).to(self.device)\n",
    "        next_states = torch.cat(batch.next_state).view(self.batch_size, self.features_dim).to(self.device)\n",
    "        return states, rewards, dones, actions, next_states\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        训练函数\n",
    "        '''\n",
    "        # 0. 经验回放缓冲区中取样\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = self.replay_buffer.sample()\n",
    "        state, reward, done, action, next_state = self.unpack(batch)\n",
    "\n",
    "        # 1. 策略网络根据state预测动作\n",
    "        action_pi, log_prob = self.policy_net(state)\n",
    "        # log_prob = log_prob.reshape(-1, 1) \n",
    "\n",
    "        # 2. 更新温度\n",
    "        ent_coef = torch.exp(self.log_ent_coef.detach())\n",
    "        ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n",
    "        self.ent_coef_opt.zero_grad()\n",
    "        ent_coef_loss.backward()\n",
    "        self.ent_coef_opt.step()\n",
    "         \n",
    "        # 3. 更新状态价值函数\n",
    "        # 3.1 计算目标Q值target\n",
    "        with torch.no_grad():   # 不更新q1_target和q2_target\n",
    "            next_action, next_log_prob = self.policy_net(next_state)\n",
    "            q1_target = self.q1_target(next_state, next_action)\n",
    "            q2_target = self.q2_target(next_state, next_action)\n",
    "            q_target = torch.min(q1_target, q2_target)\n",
    "            #! 这里next_log_prob的维度可能出现问题\n",
    "            target_q = reward + self.gamma * (1 - done) * (q_target - ent_coef * next_log_prob)\n",
    "\n",
    "        # 3.2 计算q网络的当前q值current_q1, current_q2\n",
    "        current_q1 = self.q1(state, action)\n",
    "        current_q2 = self.q2(state, action)\n",
    "\n",
    "        # 3.3 计算q网络的损失函数\n",
    "        q1_loss = F.mse_loss(current_q1, target_q)\n",
    "        q2_loss = F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # 3.4 更新q网络\n",
    "        self.q1_opt.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_opt.step()\n",
    "        self.q2_opt.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_opt.step()\n",
    "\n",
    "        # 4. 更新策略网络\n",
    "        # 4.1 计算策略网络的损失函数\n",
    "        q1 = self.q1(state, action_pi)\n",
    "        q2 = self.q2(state, action_pi)\n",
    "        q_min = torch.min(q1, q2)\n",
    "        policy_loss = (ent_coef * log_prob - q_min).mean()\n",
    "\n",
    "        # 4.2 更新策略网络\n",
    "        self.policy_opt.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_opt.step()\n",
    "\n",
    "        # 5. 软更新目标Q网络\n",
    "        for param, target_param in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "max_episodes = 500 # 迭代训练次数\n",
    "max_steps = 100    # 每个回合的最大步数\n",
    "\n",
    "# 创建对象\n",
    "agent = SAC(env, features_dim=2, action_dim=1)\n",
    "episode_rewards = []    # 只是用来看学习的成果的，和训练过程无关\n",
    "episode_steps = []\n",
    "\n",
    "# 开始迭代训练\n",
    "for episode in tqdm(range(max_episodes), file=sys.stdout):\n",
    "    # 重置环境并获取初始状态\n",
    "    state = env.reset()\n",
    "    # 初始化当前回合的奖励\n",
    "    episode_reward = 0\n",
    "    episode_step = 0\n",
    "    # 循环惊醒每一步的操作\n",
    "    for step in range(max_steps):\n",
    "        # 根据当前状态选择动作\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated = env.step(action)  # 与环境交互产生下一个状态并生成奖励等相关信息\n",
    "        done = terminated or truncated\n",
    "        # 存入经验回放缓冲区\n",
    "        agent.store(state, reward, done, action, next_state)\n",
    "        agent.train()\n",
    "        episode_reward += reward\n",
    "        episode_step += 1\n",
    "        # 更新当前状态\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    # 记录当前回合的奖励\n",
    "    episode_rewards.append(episode_reward)\n",
    "    episode_steps.append(episode_step)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        tqdm.write(\"Episode\"+str(episode)+\":\"+str(episode_reward))\n",
    "        tqdm.write(\"Episode\"+str(episode)+\":\"+str(episode_step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
