{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "# 导入基础库\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "# 导入pytorch库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# 导入绘图库\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('client')\n",
    "# # 导入julia\n",
    "# import julia\n",
    "# from julia import Main as env\n",
    "# env.include(\"env_cut_sim.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义网络常数\n",
    "LOG_STD_MAX = 2     # 最大标准差\n",
    "LOG_STD_MIN = -20   # 最小标准差\n",
    "\n",
    "## 自定义特征提取器\n",
    "class StateFeatureExtractor(nn.Module):\n",
    "    '''\n",
    "    特征提取器, 从state中提取特征向量, 并作为动作价值函数以及策略函数的输入\n",
    "    out_channels: 输出通道数\n",
    "    gm_size: global_map的形状。gm_size = [通道数，维度，维度，维度]\n",
    "    lm_size: local_map的形状。lm_size = [通道数，维度，维度]\n",
    "    cs_size: cs的形状(cs是一个向量)。cs_size = 维度\n",
    "    gm_output_dim: self.gm_extractor的输出维度\n",
    "    '''\n",
    "    def __init__(self, gm_out_channels, lm_out_channels, gm_size, lm_size, cs_size, gm_output_dim = 128, lm_output_dim = 512, cs_output_dim = 16, output_dim = 512):\n",
    "        super(StateFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.gm_in_channels = gm_size[0]    # global_map通道数\n",
    "        self.lm_in_channels = lm_size[0]    # local_map通道数\n",
    "        self.gm_volume = gm_size[1] * gm_size[2] * gm_size[3]   # global_map通道维度之外的三个维度的形状\n",
    "        self.lm_area = lm_size[1] * lm_size[2]  # local_map通道维度之外的两个维度的形状\n",
    "        self.gm_out_channels = gm_out_channels    # global_map输出通道数\n",
    "        self.lm_out_channels = lm_out_channels  # local_map输出通道数\n",
    "        self.cs_dim = cs_size       # cs(刀具位置和姿态)的维度\n",
    "        self.gm_output_dim = gm_output_dim  # self.gm_extractor的输出维度\n",
    "        self.lm_output_dim = lm_output_dim  # self.lm_extractor的输出维度\n",
    "        self.cs_output_dim = cs_output_dim  # self.cs_extractor的输出维度\n",
    "        self.output_dim = output_dim        # self.collective_extractor的输出维度\n",
    "\n",
    "        #! 定义global_map特征提取卷积层\n",
    "        self.gm_extractor = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=self.gm_in_channels, out_channels=self.gm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=self.gm_out_channels, out_channels=self.gm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.gm_out_channels * self.gm_volume, out_features=self.gm_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 定义local_map特征提取卷积层\n",
    "        self.lm_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.lm_in_channels, out_channels=self.lm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.lm_out_channels, out_channels=self.lm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.lm_out_channels * self.lm_area, self.lm_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 定义cs特征提取层\n",
    "        self.cs_extractor = nn.Sequential(\n",
    "            nn.Linear(self.cs_dim, self.cs_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 整合三个特征提取层\n",
    "        self.collective_extractor = nn.Sequential(\n",
    "            nn.Linear(self.gm_output_dim + self.lm_output_dim + self.cs_output_dim, self.output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        gm = x[0]\n",
    "        lm = x[1]\n",
    "        cs = x[2]\n",
    "        gm = self.gm_extractor(gm)\n",
    "        lm = self.lm_extractor(lm)\n",
    "        cs = self.cs_extractor(cs)\n",
    "        x = torch.cat((gm, lm, cs), dim=1)\n",
    "        x = self.collective_extractor(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "## 自定义动作价值函数\n",
    "class QNetWork(nn.Module):\n",
    "    '''\n",
    "    动作价值函数, 根据状态(state)输入和动作(action)输入, 输出动作价值\n",
    "    if_feature_extractor: 是否使用特征提取器。True: 使用特征提取器, False: 不使用特征提取器。默认不使用特征提取器\n",
    "    state_dim: 状态维度。如果采用特征提取器, 则特征提取器的输出维度output_dim=state_dim\n",
    "    action_dim: 动作维度。action_dim = [x位置变化, y位置变化, z位置变化, theta角度变化, phi角度变化, 主轴转速的变化, 进给速度的变化]\n",
    "    output_dim: 输出维度, 固定为1, 输出奖励值\n",
    "    feature_dim1: 第一个隐藏层维度\n",
    "    feature_dim2: 第二个隐藏层维度\n",
    "    '''\n",
    "    def __init__(self, if_feature_extractor = True, features_extractor: StateFeatureExtractor = None, state_dim = 512, action_dim = 7, output_dim = 1, hidden_dim1 = 256, hidden_dim2 = 128):\n",
    "        super(QNetWork, self).__init__()\n",
    "\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_extractor = features_extractor\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "\n",
    "        #! Q值网络\n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.action_dim, self.hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim1, self.hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim2, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        if self.if_feature_extractor:   # 采用特征提取器, 将state转换成特征向量state\n",
    "            #* 动作价值函数与策略函数共用一个特征提取器\n",
    "            with torch.no_grad():\n",
    "                state = self.features_extractor(state)\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = self.q_network(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "## 自定义策略函数\n",
    "class PolicyNetWork(nn.Module):\n",
    "    '''\n",
    "    策略函数, 根据状态(state)输入, 输出动作(action)概率\n",
    "    if_feature_extractor: 是否使用特征提取器。True: 使用特征提取器, False: 不使用特征提取器。默认不使用特征提取器\n",
    "    state_dim: 状态维度\n",
    "    action_dim: 动作维度\n",
    "    feature_dim1: 第一个隐藏层维度\n",
    "    feature_dim2: 第二个隐藏层维度\n",
    "    '''\n",
    "    def __init__(self, if_feature_extractor = True, features_extractor: StateFeatureExtractor = None, state_dim = 512, action_dim = 7, hidden_dim = 256, output_dim = 128):\n",
    "        super(PolicyNetWork, self).__init__()\n",
    "\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_extractor = features_extractor\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        #! 策略网络\n",
    "        self.policy_dist = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 输出均值和标准差\n",
    "        # TODO：https://arxiv.org/abs/2005.05719 提出一种根据状态的采样策略，可以尝试使用\n",
    "        self.log_std = nn.Linear(self.output_dim, self.action_dim)\n",
    "        self.mu = nn.Linear(self.output_dim, self.action_dim)\n",
    "\n",
    "    def forward(self, state, deterministic = False):\n",
    "        '''\n",
    "        前向传播\n",
    "        state: 状态输入\n",
    "        deterministic: 是否确定性输出，True: 输出均值，False: 输出均值和标准差的采样结果\n",
    "        '''\n",
    "        if self.if_feature_extractor:   # 使用特征提取器, 将state转换成特征向量state\n",
    "            #* 动作价值函数与策略函数共用一个特征提取器, 并且特征提取器在\n",
    "            state = self.features_extractor(state)\n",
    "        x = self.policy_dist(state)\n",
    "        mu = self.mu(x)\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        else:\n",
    "            log_std = self.log_std(x)\n",
    "            log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "            std = torch.exp(log_std)\n",
    "            dist = torch.distributions.Normal(mu, std)\n",
    "            return dist.rsample()\n",
    "        \n",
    "    def mu_and_log_std(self, state, deterministic = False):\n",
    "        '''\n",
    "        输出均值和标准差，用于policy的训练\n",
    "        state: 状态输入\n",
    "        '''\n",
    "        # 0. 特征提取器提取特征\n",
    "        if self.if_feature_extractor:\n",
    "            state = self.features_extractor(state)\n",
    "        # 1. 通过policy网络得到log_std和mu\n",
    "        x = self.policy_dist(state)\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)    # 限制log_std的范围\n",
    "        std = torch.exp(log_std)    # 得到真正的标准差\n",
    "        # 2. 通过std和mu创建高斯分布\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        # 3. 返回从高斯分布中采样的动作\n",
    "        action = dist.rsample()\n",
    "        # 4. 计算动作分布的标准差并返回对数标准差的和\n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob -= torch.log(1 - action ** 2 + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        # 5. 将动作限制在[-1, 1]之间\n",
    "        action = torch.tanh(action)\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 3, 3, 3])\n",
      "torch.Size([3, 3, 3, 3, 3])\n",
      "torch.Size([3])\n",
      "torch.Size([128, 315])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "f = StateFeatureExtractor(gm_out_channels=3, lm_out_channels=7, gm_size=[2, 3, 5,7], lm_size=[7, 23, 23], cs_size=9)\n",
    "for prar in f.parameters():\n",
    "    print(prar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 经验回放缓冲区\n",
    "# TODO: 优化经验回放缓冲区\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state','reward', 'done', 'action', 'next_state'))\n",
    "\n",
    "#// class Memory:\n",
    "#//     def __init__(self, memory_size):\n",
    "#//         self.memory_size = memory_size\n",
    "#//         self.memory = []\n",
    "#//     def add(self, *transition):\n",
    "#//         self.memory.append(Transition(*transition))\n",
    "#//         if len(self.memory) > self.memory_size:\n",
    "#//             self.memory.pop(0)\n",
    "#//         assert len(self.memory) <= self.memory_size\n",
    "#//     def sample(self, batch_size = 256):\n",
    "#//         return random.sample(self.memory, batch_size)\n",
    "#//     def __len__(self):\n",
    "#//         return len(self.memory)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    '''\n",
    "    经验回放缓冲区\n",
    "    memory_size: 缓冲区大小\n",
    "    batch_size: 批次大小\n",
    "    '''\n",
    "    def __init__(self, memory_size = 1000000, batch_size = 256):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "\n",
    "    def add(self, state, reward, done, action, next_state):\n",
    "        self.memory.append(Transition(state, reward, done, action, next_state))\n",
    "        if len(self.memory) > self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "        assert len(self.memory) <= self.memory_size\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAC算法\n",
    "class SAC:\n",
    "    '''\n",
    "    learning_rate: 学习率\n",
    "    tau: 软更新参数\n",
    "    gamma: 折扣因子\n",
    "    buffer_size: 经验回放缓冲区大小\n",
    "    batch_size: 训练batch大小\n",
    "    if_feature_extractor: 是否使用特征提取器\n",
    "    gm_out_channels: global_map输出通道数\n",
    "    lm_out_channels: local_map输出通道数\n",
    "    gm_size: global_map的形状。gm_size = [通道数，维度，维度，维度]\n",
    "    lm_size: local_map的形状。lm_size = [通道数，维度，维度]\n",
    "    cs_size: cs的形状(cs是一个向量)。cs_size = 维度\n",
    "    gm_output_dim: self.gm_extractor的输出维度\n",
    "    lm_output_dim: self.lm_extractor的输出维度\n",
    "    cs_output_dim: self.cs_extractor的输出维度\n",
    "    features_dim: 特征提取器的输出维度\n",
    "    action_dim: 动作维度\n",
    "    q_hidden_dim1: Q网络第一个隐藏层维度\n",
    "    q_hidden_dim2: Q网络第二个隐藏层维度\n",
    "    p_hidden_dim1: 策略网络第一个隐藏层维度\n",
    "    p_hidden_dim2: 策略网络第二个隐藏层维度\n",
    "    ent_coef: 熵系数\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            learning_rate = 0.003,\n",
    "            tau = 0.005,\n",
    "            gamma = 0.99,\n",
    "            buffer_size = 1000000,\n",
    "            batch_size = 256,\n",
    "            if_feature_extractor = False,\n",
    "            gm_out_channels = 2,\n",
    "            lm_out_channels = 18,\n",
    "            gm_size = [2, 10, 10, 10],\n",
    "            lm_size = [18, 75, 75],\n",
    "            cs_size = 9,\n",
    "            gm_output_dim = 128,\n",
    "            lm_output_dim = 512,\n",
    "            cs_output_dim = 16,\n",
    "            features_dim = 512,\n",
    "            action_dim = 7,\n",
    "            q_hidden_dim1 = 256,\n",
    "            q_hidden_dim2 = 128,\n",
    "            p_hidden_dim1 = 256,\n",
    "            p_hidden_dim2 = 128,\n",
    "            ent_coef = 0.1\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_dim = features_dim    # 特征提取器的输出维度，同时也是Q网络和策略网络的状态输入维度\n",
    "        self.action_dim = action_dim        # 动作维度\n",
    "        self.q_hidden_dim1 = q_hidden_dim1  # Q网络第一个隐藏层维度\n",
    "        self.q_hidden_dim2 = q_hidden_dim2\n",
    "        self.p_hidden_dim1 = p_hidden_dim1\n",
    "        self.p_hidden_dim2 = p_hidden_dim2\n",
    "        self.target_entropy = -self.action_dim  # 目标熵\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # 使用特征提取器\n",
    "        if if_feature_extractor:\n",
    "            self.gm_out_channels = gm_out_channels\n",
    "            self.lm_out_channels = lm_out_channels\n",
    "            self.gm_size = gm_size\n",
    "            self.lm_size = lm_size\n",
    "            self.cs_size = cs_size\n",
    "            self.gm_output_dim = gm_output_dim\n",
    "            self.lm_output_dim = lm_output_dim\n",
    "            self.cs_output_dim = cs_output_dim\n",
    "            self.features_extractor = StateFeatureExtractor(gm_output_channels=self.gm_out_channels, lm_out_channels=self.lm_out_channels, \n",
    "                                                            gm_size=self.gm_size, lm_size=self.lm_size, cs_size=self.cs_size, \n",
    "                                                            gm_output_dim=self.gm_output_dim, lm_output_dim=self.lm_output_dim, cs_output_dim=self.cs_output_dim, output_dim=self.features_dim).to(self.device)\n",
    "        else:\n",
    "            self.features_extractor = None\n",
    "\n",
    "        # 定义Q网络\n",
    "        self.q1 = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor, \n",
    "                      state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q2 = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                      state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q1_target = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                             state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q2_target = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                             state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q1_opt = optim.Adam(self.q1.parameters(), lr=self.learning_rate)   # 定义Q网络优化器\n",
    "        self.q2_opt = optim.Adam(self.q2.parameters(), lr=self.learning_rate)   # 定义Q网络优化器\n",
    "\n",
    "        # 定义策略网络\n",
    "        self.policy_net = PolicyNetWork(state_dim=self.features_dim, action_dim=self.action_dim, \n",
    "                                        if_feature_extractor=self.if_feature_extractor, \n",
    "                                        hidden_dim=self.p_hidden_dim1, output_dim=self.p_hidden_dim2).to(self.device)\n",
    "        if self.if_feature_extractor:   # 采用特征提取器, 则策略网络和特征提取器共同训练\n",
    "            self.policy_opt = optim.Adam(list(self.features_extractor.parameters() + list(self.policy_net.parameters())), lr=self.learning_rate)\n",
    "        else:\n",
    "            self.policy_opt = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)   # 定义策略网络优化器\n",
    "        \n",
    "        # 定义经验回放缓冲区\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        # 定义温度\n",
    "        self.log_ent_coef = torch.tensor([0.0]).to(self.device).requires_grad_()\n",
    "        self.ent_coef_opt = optim.Adam([self.log_ent_coef], lr=self.learning_rate)\n",
    "\n",
    "    def choose_action(self, state, deterministic = True):\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = self.policy_net(state, deterministic = deterministic)\n",
    "        return action.cpu().detach().numpy()\n",
    "\n",
    "    def store(self, state, reward, done, action, next_state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        reward = torch.Tensor([reward])\n",
    "        done = torch.Tensor([done])\n",
    "        action = np.array([action])\n",
    "        action = torch.Tensor(action)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        self.replay_buffer.add(state, reward, done, action, next_state)\n",
    "\n",
    "    def unpack(self, batch):\n",
    "        batch = Transition(*zip(*batch))\n",
    "        states = torch.cat(batch.state).view(self.batch_size, self.features_dim).to(self.device)\n",
    "        rewards = torch.cat(batch.reward).view(self.batch_size, 1).to(self.device)\n",
    "        dones = torch.cat(batch.done).view(self.batch_size, 1).to(self.device)\n",
    "        actions = torch.cat(batch.action).view(-1, self.action_dim).to(self.device)\n",
    "        next_states = torch.cat(batch.next_state).view(self.batch_size, self.features_dim).to(self.device)\n",
    "        return states, rewards, dones, actions, next_states\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        训练函数\n",
    "        '''\n",
    "        # 0. 经验回放缓冲区中取样\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = self.replay_buffer.sample()\n",
    "        state, reward, done, action, next_state = self.unpack(batch)\n",
    "\n",
    "        # 1. 策略网络根据state预测动作\n",
    "        action, log_prob = self.policy_net.mu_and_log_std(state)\n",
    "        # log_prob = log_prob.reshape(-1, 1) \n",
    "\n",
    "        # 2. 更新温度\n",
    "        ent_coef = torch.exp(self.log_ent_coef.detach())\n",
    "        ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n",
    "        self.ent_coef_opt.zero_grad()\n",
    "        ent_coef_loss.backward()\n",
    "        self.ent_coef_opt.step()\n",
    "         \n",
    "        # 3. 更新状态价值函数\n",
    "        # 3.1 计算目标Q值target\n",
    "        with torch.no_grad():   # 不更新q1_target和q2_target\n",
    "            next_action, next_log_prob = self.policy_net.mu_and_log_std(next_state)\n",
    "            q1_target = self.q1_target(next_state, next_action)\n",
    "            q2_target = self.q2_target(next_state, next_action)\n",
    "            q_target = torch.min(q1_target, q2_target)\n",
    "            #! 这里next_log_prob的维度可能出现问题\n",
    "            target_q = reward + self.gamma * (1 - done) * (q_target - ent_coef * next_log_prob)\n",
    "\n",
    "        # 3.2 计算q网络的当前q值current_q1, current_q2\n",
    "        current_q1 = self.q1(state, action)\n",
    "        current_q2 = self.q2(state, action)\n",
    "\n",
    "        # 3.3 计算q网络的损失函数\n",
    "        q1_loss = F.mse_loss(current_q1, target_q)\n",
    "        q2_loss = F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # 3.4 更新q网络\n",
    "        self.q1_opt.zero_grad()\n",
    "        q1_loss.backward(retain_graph=True)\n",
    "        self.q1_opt.step()\n",
    "        self.q2_opt.zero_grad()\n",
    "        q2_loss.backward(retain_graph=True)\n",
    "        self.q2_opt.step()\n",
    "\n",
    "        # 4. 更新策略网络\n",
    "        # 4.1 计算策略网络的损失函数\n",
    "        q1 = self.q1(state, action)\n",
    "        q2 = self.q2(state, action)\n",
    "        q_min = torch.min(q1, q2)\n",
    "        policy_loss = (ent_coef * log_prob - q_min).mean()\n",
    "\n",
    "        # 4.2 更新策略网络\n",
    "        self.policy_opt.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_opt.step()\n",
    "\n",
    "        # 5. 软更新目标Q网络\n",
    "        for param, target_param in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具类，用于显示渲染结果\n",
    "from IPython import display # 导入display模块，用于在Jupyter notebook中显示图像\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GymHelper:\n",
    "    def __init__(self, env, figsize=(3,3)):\n",
    "        self.env = env\n",
    "        self.figsize = figsize\n",
    "        plt.figure(figsize = figsize)\n",
    "        self.img = plt.imshow(env.render())\n",
    "\n",
    "    def render(self, title = None):\n",
    "        image_data = self.env.render()\n",
    "        self.img.set_data(image_data)   # 更新绘图窗口中的图像数据\n",
    "        display.display(plt.gcf())      # 刷新显示\n",
    "        display.clear_output(wait = True)   #有新图像时再清楚绘图窗口原有的图像\n",
    "        if title:   # 如果有标题，就显示标题\n",
    "            plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Box2D is not installed, run `pip install gymnasium[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:15\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 创建CartPole环境，指定渲染模式为rgb_array，如果是在IDE中可以改为“human”\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLunarLanderContinuous-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 重置环境\u001b[39;00m\n\u001b[0;32m      8\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\gymnasium\\envs\\registration.py:756\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    753\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[0;32m    759\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\gymnasium\\envs\\registration.py:545\u001b[0m, in \u001b[0;36mload_env_creator\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m \n\u001b[0;32m    538\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 545\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\gymnasium\\envs\\box2d\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\gymnasium\\envs\\box2d\\bipedal_walker.py:25\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m         circleShape,\n\u001b[0;32m     18\u001b[0m         contactListener,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m         revoluteJointDef,\n\u001b[0;32m     23\u001b[0m     )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox2D is not installed, run `pip install gymnasium[box2d]`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Box2D is not installed, run `pip install gymnasium[box2d]`"
     ]
    }
   ],
   "source": [
    "## 测试\n",
    "# 导入gym环境\n",
    "import gymnasium as gym\n",
    "\n",
    "# 创建CartPole环境，指定渲染模式为rgb_array，如果是在IDE中可以改为“human”\n",
    "env = gym.make('LunarLanderContinuous-v2', render_mode='rgb_array', continuous = True)\n",
    "# 重置环境\n",
    "env.reset()\n",
    "# 创建GymHelper对象\n",
    "gym_helper = GymHelper(env)\n",
    "\n",
    "# 循环N次\n",
    "N = 100\n",
    "for i in range(N):\n",
    "    gym_helper.render(title = str(i))    # 渲染环境\n",
    "    action = env.action_space.sample()  #从动作空间中随机选取一个动作\n",
    "    env.step(action)    # 执行这个动作\n",
    "\n",
    "# 关闭环境\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Dimension: 2\n",
      "State Dimension: 8\n"
     ]
    }
   ],
   "source": [
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "print(f\"Action Dimension: {action_dim}\")\n",
    "print(f\"State Dimension: {state_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 循环惊醒每一步的操作\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# 根据当前状态选择动作\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# 与环境交互产生下一个状态并生成奖励等相关信息\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[1;32mIn[16], line 113\u001b[0m, in \u001b[0;36mSAC.choose_action\u001b[1;34m(self, state, deterministic)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    112\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 113\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 191\u001b[0m, in \u001b[0;36mPolicyNetWork.forward\u001b[1;34m(self, state, deterministic)\u001b[0m\n\u001b[0;32m    189\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[0;32m    190\u001b[0m log_prob \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m action \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m--> 191\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mlog_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# 5. 将动作限制在[-1, 1]之间\u001b[39;00m\n\u001b[0;32m    193\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(action)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# 定义参数\n",
    "max_episodes = 500 # 迭代训练次数\n",
    "max_steps = 1000    # 每个回合的最大步数\n",
    "\n",
    "# 创建对象\n",
    "agent = SAC(env, features_dim=state_dim, action_dim=action_dim)\n",
    "episode_rewards = []    # 只是用来看学习的成果的，和训练过程无关\n",
    "\n",
    "# 开始迭代训练\n",
    "for episode in tqdm(range(max_episodes), file=sys.stdout):\n",
    "    # 重置环境并获取初始状态\n",
    "    state, _ = env.reset()\n",
    "    # 初始化当前回合的奖励\n",
    "    episode_reward = 0\n",
    "\n",
    "    # 循环惊醒每一步的操作\n",
    "    for step in range(max_steps):\n",
    "        # 根据当前状态选择动作\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)  # 与环境交互产生下一个状态并生成奖励等相关信息\n",
    "        done = terminated or truncated\n",
    "        # 存入经验回放缓冲区\n",
    "        agent.store(state, reward, done, action, next_state)\n",
    "        agent.train()\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 更新当前状态\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    # 记录当前回合的奖励\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        tqdm.write(\"Episode\"+str(episode)+\":\"+str(episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 重置环境开始新一轮的游戏\n",
    "observation, _ = env.reset()\n",
    "# 创建GymHelper对象\n",
    "gym_helper = GymHelper(env, figsize = (3,3))\n",
    "\n",
    "# 循环N次\n",
    "N = 300\n",
    "for i in range(N):\n",
    "    gym_helper.render(title = str(i))    # 渲染环境\n",
    "    action = agent.choose_action(observation, True)\n",
    "    observation, reward, termintated, truncated, info = env.step(action)\n",
    "\n",
    "# 关闭环境\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
